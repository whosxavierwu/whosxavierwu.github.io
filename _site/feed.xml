<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.2">Jekyll</generator><link href="http://localhost:4034/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4034/" rel="alternate" type="text/html" /><updated>2020-04-14T16:44:31+08:00</updated><id>http://localhost:4034/</id><title type="html">whosxavierwu’s blog</title><subtitle>Keep learning, deep learning. </subtitle><entry><title type="html">Deep Neural Network for YouTube Recommendation System</title><link href="http://localhost:4034/recommender/2020/04/13/dnn-for-youtube-recommend.html" rel="alternate" type="text/html" title="Deep Neural Network for YouTube Recommendation System" /><published>2020-04-13T00:00:00+08:00</published><updated>2020-04-13T00:00:00+08:00</updated><id>http://localhost:4034/recommender/2020/04/13/dnn-for-youtube-recommend</id><content type="html" xml:base="http://localhost:4034/recommender/2020/04/13/dnn-for-youtube-recommend.html">&lt;h1 id=&quot;概述&quot;&gt;概述&lt;/h1&gt;

&lt;p&gt;这篇论文是对YouTube中基于DNN的推荐系统的整体描述。本文旨在对论文进行总结。&lt;/p&gt;

&lt;p&gt;整个系统主要分 Candidate Generation 和 Ranking，也就是召回和排序两部分。召回模块从数百万的视频集合中挑选出数百个候选视频；排序模块则是从数百个中挑选出几十个视频，并排序后推送给用户。&lt;/p&gt;

&lt;p&gt;下图是整体框架：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4034/assets/youtube-dnn-whole.jpg&quot; alt=&quot;整体框架&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;一召回&quot;&gt;一、召回&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4034/assets/youtube-dnn-candidate-generate.jpg&quot; alt=&quot;Candidate Generation&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-问题定义&quot;&gt;1. 问题定义&lt;/h2&gt;

&lt;p&gt;YouTube将召回问题转化为一个多分类问题去处理，建模以预测：在$t$时刻发生的某次视频观看事件$w_t$中，具体观看的是视频集合$V$中的哪个视频。&lt;/p&gt;

&lt;p&gt;假设用$U$表示这次事件中的用户，用$C$表示上下文，用$u$表示对$U$、$C$一起进行embedding后的特征向量，用$v_i$表示对视频$i$进行embedding后的特征向量。则我们需要预测的分类到视频$i$的概率可以形式化如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(w_t=i|U,C)=\frac{e^{v_iu}}{\sum_{j\in V}e^{v_ju}}&lt;/script&gt;

&lt;p&gt;通过这样的问题转化以后，理想情况下，我们能预测到在当前用户、当前情景下，每个视频被观看的概率。取概率最高的前M个视频即可作为召回模块的输出。&lt;/p&gt;

&lt;h2 id=&quot;2-数据准备&quot;&gt;2. 数据准备&lt;/h2&gt;

&lt;p&gt;在准备数据样本时，需要注意：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;数据源方面，采用所有YouTube视频的观看事件（如嵌在其他网站的视频等），而不仅仅是YouTube主站上的。&lt;/li&gt;
  &lt;li&gt;对每个用户带来的训练样本数进行了限制，从而避免高活跃用户对模型的过度影响。&lt;/li&gt;
  &lt;li&gt;注意避免样本数据中掺入未来信息，模型的输入应该始终只有打标签以前的数据。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4034/assets/youtube-dnn-dataset.jpg&quot; alt=&quot;训练数据筛选&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-特征处理&quot;&gt;3. 特征处理&lt;/h2&gt;

&lt;p&gt;特征方面，抽象来看，主要涉及用户属性、用户行为与事件时间特征三大块。作者在论文中给出了不同特征组合的效果对比：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4034/assets/youtube-dnn-feature-select.jpg&quot; alt=&quot;Features&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;31-用户属性特征&quot;&gt;3.1 用户属性特征&lt;/h3&gt;

&lt;p&gt;用户属性特征在论文中只是简单的一笔带过，包括：用户处理的地理位置、设备、性别、登录状态、年龄。直觉来看，这类特征应该对新用户的推荐效果有着重要影响。&lt;/p&gt;

&lt;p&gt;尽管没有细讲，从最后对不同特征组合的实验来看，却似乎带有很大的提升：”All Features”相对于”Watches, Searches &amp;amp; Example Age”有显著提升。&lt;/p&gt;

&lt;h3 id=&quot;32-用户行为特征&quot;&gt;3.2 用户行为特征&lt;/h3&gt;

&lt;p&gt;对用户行为的特征挖掘，主要从用户的视频观看历史（”watch vector”）与搜索历史（”search vector”）着手。&lt;/p&gt;

&lt;h4 id=&quot;watch-vector&quot;&gt;“watch vector”&lt;/h4&gt;

&lt;p&gt;从用户的视频观看历史中挖掘特征主要分两步：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;通过单独的模型预训练好每个视频的embedding。&lt;/li&gt;
  &lt;li&gt;取出用户历史（&lt;em&gt;All or Top-k?&lt;/em&gt;）观看的视频的embedding取均值，作为 “watch vectors”。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;具体而言，是如何做embedding的呢？文中只是简单的提了一下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Inspired by continuous bag of words language models, we learn high dimensional embeddings for each video in a fixed vocabulary&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从我的理解来看，应该是将每个用户历史观看的视频ID序列，看作一个“句子”，所有用户的“句子”汇聚成一个语料集合；进而参考Word2Vec中基于CBOW的训练方式来做训练，从而获得每个视频的embedding。&lt;/p&gt;

&lt;p&gt;作者还提到了一句：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Importantly, the embeddings are learned jointly with all other model parameters through normal gradient descent backpropagation updates&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这个我不太理解。&lt;/p&gt;

&lt;h4 id=&quot;search-vector&quot;&gt;“search vector”&lt;/h4&gt;

&lt;p&gt;从用户的搜索历史中挖掘特征的步骤，与前面相似：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;将每个query分词成unigrams跟bigrams，而token又是被embedding好的，&lt;/li&gt;
  &lt;li&gt;汇总所有的这些embedding求均值，作为 “search vector”&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;Search history is treated similarly to watch history - each query is tokenized into unigrams and bigrams and each token is embedded. Once averaged, the user’s tokenized, embedded queries represent a summarized dense search history&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;从作者的描述来看，应该就是基于用户的搜索预料来训练Word2Vec模型，从而得到embedding向量。&lt;/p&gt;

&lt;h3 id=&quot;33-事件时间特征&quot;&gt;3.3 事件时间特征&lt;/h3&gt;

&lt;p&gt;“Example Age” 是个较为特殊的特征。引入这个特征，是因为作者观察到，用户更偏好新产的视频。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;we feed the age of the training example as a feature during training. At serving time, this feature is set to zero (or slightly negative) to reflect that the model is making predictions at the very end of the training window.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;论文在一张插图的描述中提到：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;the example age is expressed as $t_{max} - t_N$ where $t_{max}$ is the maximum observed time in the training data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$t_N$指的是样本打标签的时间，也就是当前的事件的时间戳，这个好理解。&lt;/p&gt;

&lt;p&gt;虽然说得比较模糊，但结合前面的描述：在serving时，该特征被置为零。所以$t_{max}$应该是指全体训练样本中的最大观测时间。&lt;/p&gt;

&lt;p&gt;至于具体是用秒？分钟？小时？还是天？则没有提及，考虑到不同量纲之间可以通过线性变换来相互切换，所以这个问题的影响不大。&lt;/p&gt;

&lt;p&gt;作者通过统计分析表明，模型在加入了”Example Age”之后，能比较好的捕捉到视频上传时间的影响。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4034/assets/youtube-dnn-example-age.jpg&quot; alt=&quot;Example Age&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么问题来了，为什么不直接用”Days Since Upload”来做特征呢？&lt;/p&gt;

&lt;h2 id=&quot;4-模型训练与线上服务&quot;&gt;4. 模型训练与线上服务&lt;/h2&gt;

&lt;h3 id=&quot;41-训练技巧-negative-sampling&quot;&gt;4.1 训练技巧： Negative Sampling&lt;/h3&gt;

&lt;p&gt;一般情况下，基于 SoftMax 的 Cross-Entropy Loss 形式如下：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;logit(i)=\frac{exp(w_{i}x)}{\sum^{M}_{j}{exp({w_{j}x})}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;loss=-log(logit(i))=-(w_ix)+log(\sum^{M}_{j}{exp(w_jx)})&lt;/script&gt;

&lt;p&gt;可以看到，当类别数$M$多达数百万的时候，损失函数的后半部分$ log(\sum^{M}_{j}exp(w_jx)) $的计算量将会特别大。&lt;/p&gt;

&lt;p&gt;而 Negative Sampling 的思路则是，通过采样指定$K$个类别，从而把计算量从$O(M) \to O(K)$控制了下来。作者在论文中指出，一般$K$取数千。&lt;/p&gt;

&lt;p&gt;这里有几个细节：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$K$是否把类别$i$包含在内？&lt;/li&gt;
  &lt;li&gt;具体如何进行随机采样？均匀采样？&lt;/li&gt;
  &lt;li&gt;是每个训练样本都做一次采样？还是每个batch做一次采样？&lt;/li&gt;
  &lt;li&gt;每次负采样、训练时，并不会更新$K$个被选中的类别以外的类别权重。那么如果存在某个类别的样本数量相对较大，会不会对模型效果有影响？&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;42-线上服务&quot;&gt;4.2 线上服务&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4034/assets/youtube-dnn-recall-serving.jpg&quot; alt=&quot;Candidate Generation Serving&quot; /&gt;&lt;/p&gt;

&lt;p&gt;模型框架图中的这个细节，是我一开始没有留意到的。&lt;/p&gt;

&lt;p&gt;当时只是想当然的认为，在做serving时，每次用户来到时，跑一遍模型预测，然后取出概率值Top N的视频来召回。而从YouTube的框架图来看，实际做serving时是以下步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;从最后一层ReLU层获取用户向量$\vec{u}$（256维）；&lt;/li&gt;
  &lt;li&gt;从SoftMax层获取视频向量$\vec{v_j}$；&lt;/li&gt;
  &lt;li&gt;通过最近邻搜索来找到近似的Top N视频。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;以上的简要描述可能仍然不好理解。&lt;/p&gt;

&lt;p&gt;我们知道，在ReLU和SoftMax两层之间存在一个大小为$(256, V)$的权重矩阵$\vec{W}$，$V$表示视频总数；$\vec{W}$通过训练学习到。&lt;/p&gt;

&lt;p&gt;来看常规的feedward流程：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;计算至最后的ReLU层得到$\vec{u}$；&lt;/li&gt;
  &lt;li&gt;进行矩阵乘法$\vec{z}=\vec{u}^T\vec{W}$；&lt;/li&gt;
  &lt;li&gt;进行指数运算$exp(\vec{z})$；&lt;/li&gt;
  &lt;li&gt;归一化$\vec{y}=exp(\vec{z})/||exp(\vec{z})||_1$；&lt;/li&gt;
  &lt;li&gt;按$y_j$进行倒序取Top-N作为召回结果；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;观察到，由于指数运算具有单调性，且在进行召回时只关注模型输出的相对值，而不关注绝对值；我们发现3、4两步可以省略掉，直接在计算出${\vec{z}}$之后，取$z_j$的值来作为排序的依据即可。&lt;/p&gt;

&lt;p&gt;由于视频数量巨大，$\vec{z}=\vec{u}^T\vec{W}$这一步仍然存在高昂的计算成本。为了提升效率，在完成了模型训练之后，可以提前把$\vec{W}$拆成一个个列向量$\vec{v_j}$。&lt;/p&gt;

&lt;p&gt;线上serving时，计算出用户向量$\vec{u}$之后，下一步就变成了寻找与$\vec{u}$内积最大的N个列向量${\vec{v_j}}$的问题。而这可以转化为最近邻搜索问题（作者引用论文：&lt;a href=&quot;http://www.cs.cmu.edu/~agray/approxnn.pdf&quot;&gt;An investigation of practical approximate nearest neighbor&lt;/a&gt;）。&lt;/p&gt;

&lt;h1 id=&quot;二排序&quot;&gt;二、排序&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4034/assets/youtube-dnn-ranking.jpg&quot; alt=&quot;Ranking&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-问题定义-1&quot;&gt;1. 问题定义&lt;/h2&gt;

&lt;p&gt;YouTube的推荐系统中，将排序问题转化为一个对视频观看时长的预测问题。&lt;/p&gt;

&lt;h2 id=&quot;2-数据&quot;&gt;2. 数据&lt;/h2&gt;

&lt;h2 id=&quot;3-特征&quot;&gt;3. 特征&lt;/h2&gt;

&lt;h3 id=&quot;31-video-embedding&quot;&gt;3.1. “video embedding”&lt;/h3&gt;

&lt;h3 id=&quot;32-language-embedding&quot;&gt;3.2. “language embedding”&lt;/h3&gt;

&lt;h3 id=&quot;33-time-since-last-watch&quot;&gt;3.3. “time since last watch”&lt;/h3&gt;

&lt;h3 id=&quot;34-number-of-previous-impressions&quot;&gt;3.4. “number of previous impressions”&lt;/h3&gt;

&lt;h1 id=&quot;参考&quot;&gt;参考&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/2959100.2959190&quot;&gt;论文地址&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52169807&quot;&gt;王喆-1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/52504407&quot;&gt;王喆-十个工程问题&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/61827629&quot;&gt;王喆-模型Serving&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/38638747&quot;&gt;工程再现&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">概述</summary></entry><entry><title type="html">Data Split</title><link href="http://localhost:4034/2019/10/07/data-split.html" rel="alternate" type="text/html" title="Data Split" /><published>2019-10-07T00:00:00+08:00</published><updated>2019-10-07T00:00:00+08:00</updated><id>http://localhost:4034/2019/10/07/data-split</id><content type="html" xml:base="http://localhost:4034/2019/10/07/data-split.html">&lt;h1 id=&quot;数据集划分方式&quot;&gt;数据集划分方式&lt;/h1&gt;

&lt;p&gt;训练集、验证集、测试集、线上数据&lt;/p&gt;

&lt;p&gt;比赛时，我们的目标是尽可能的在测试集上拟合，至于测试集跟线上数据之间的偏差如何，参赛者可以不考虑在内。&lt;/p&gt;

&lt;p&gt;而实际工作中，我们需要在线上数据中拟合得好。&lt;/p&gt;

&lt;p&gt;从某种角度来看，实际工作中，测试集就是线上数据。&lt;/p&gt;

&lt;p&gt;无论如何，测试集的特点就在于——我们完全看不到。&lt;/p&gt;

&lt;p&gt;所以在建模时，我们实际上看到的是训练集+验证集的总和。&lt;/p&gt;

&lt;p&gt;为什么需要单独拎出来一个验证集？&lt;/p&gt;

&lt;p&gt;是为了线下简单验证模型的效果，如果能在线上数据直接验证模型效果那当然是最理想的了。&lt;/p&gt;

&lt;p&gt;如果可见集（训练集+验证集）的分布跟不可见集（线上数据或测试集）的分布完全一致。&lt;/p&gt;

&lt;p&gt;那随便一个模型都能打出完美的效果。&lt;/p&gt;

&lt;p&gt;正因为分布不同，我们才需要划出来一个验证集，就是因为我们希望、也认为，验证集跟测试集相对接近。&lt;/p&gt;

&lt;p&gt;所以在对可见集进行划分的时候，最最重要的是，想办法让验证集跟测试集的分布相似。&lt;/p&gt;

&lt;p&gt;可是我们又完全看不到测试集长什么样，该怎么办呢？&lt;/p&gt;

&lt;p&gt;于是有两种：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;完全随机的抽出来作为验证集&lt;/li&gt;
  &lt;li&gt;让验证集的分布跟可见集的分布接近&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">数据集划分方式</summary></entry><entry><title type="html">Qa Logistic Regression</title><link href="http://localhost:4034/2019/10/05/qa-logistic-regression.html" rel="alternate" type="text/html" title="Qa Logistic Regression" /><published>2019-10-05T00:00:00+08:00</published><updated>2019-10-05T00:00:00+08:00</updated><id>http://localhost:4034/2019/10/05/qa-logistic-regression</id><content type="html" xml:base="http://localhost:4034/2019/10/05/qa-logistic-regression.html">&lt;h1 id=&quot;qa-logistic-regression&quot;&gt;Q&amp;amp;A: Logistic Regression&lt;/h1&gt;

&lt;p&gt;Q1: 与线性回归相比，有何异同？&lt;/p&gt;

&lt;p&gt;Q2: 如何用于多标签分类问题？&lt;/p&gt;

&lt;p&gt;Q3: 线性回归在实现时是如何求解的？也是用梯度下降？&lt;/p&gt;</content><author><name></name></author><summary type="html">Q&amp;amp;A: Logistic Regression</summary></entry><entry><title type="html">自动文本摘要</title><link href="http://localhost:4034/text-summarization/2019/03/15/automatic-text-summarization.html" rel="alternate" type="text/html" title="自动文本摘要" /><published>2019-03-15T17:00:00+08:00</published><updated>2019-03-15T17:00:00+08:00</updated><id>http://localhost:4034/text-summarization/2019/03/15/automatic-text-summarization</id><content type="html" xml:base="http://localhost:4034/text-summarization/2019/03/15/automatic-text-summarization.html">&lt;p&gt;最近因为项目需要，加上个人兴趣，就找了几篇有关自动文本摘要的综述论文，简单做了些调研、学习，这里做个小结。&lt;/p&gt;

&lt;h1 id=&quot;0概述&quot;&gt;0、概述&lt;/h1&gt;

&lt;p&gt;自动文本摘要，顾名思义，就是自动的从文本中生成摘要。&lt;/p&gt;

&lt;p&gt;其实这方面的研究最早在1950年代就开始了，但一直得不到大众的关注，在国际上尚且如此，在国内的情况就更加尴尬了。基于中文文本的自动摘要，在国内能找到的资料、工具并不多。&lt;/p&gt;

&lt;h1 id=&quot;1应用场景&quot;&gt;1、应用场景&lt;/h1&gt;

&lt;p&gt;自动文本摘要能用在哪些地方呢？从个人习惯来说，我倾向于把日常接触到的文字内容分为“水货”和“干货”两种。&lt;/p&gt;

&lt;p&gt;“水货”是指哪些呢？就是那些看完之后没有太多实际收获的内容，比如很多公众号里的鸡汤文、鸡血文、狗血文等等，对于很多人而言，小说类侧重故事性的文章，也都是“水货”。总的来说，我们发现，其实自动文本摘要并不适用于这一类内容，因为读者接触这些内容是为了来体验、来感受的，而不是抱着功利心来的。&lt;/p&gt;

&lt;p&gt;自动文本摘要主要还是应用于“干货”上，新闻文章、科学文献、股市研报等等，人们从这些内容中，想要的是新的信息收获。&lt;/p&gt;

&lt;p&gt;那沿着这个思路，我们就可以想到自动摘要的应用场景都有哪些了。&lt;/p&gt;

&lt;p&gt;用在新闻文章上，我们可以看到新闻快报、头条，像我几乎每天上班路上都会打开的虎嗅早报、36氪早报，以及微博上的新闻推送，其实是能通过自动摘要来生成的。&lt;/p&gt;

&lt;p&gt;用在科学文献上，我们更快速的看论文，涨知识。&lt;/p&gt;

&lt;p&gt;用在股市研报上，我们看一眼就能知道A股又要跌多少。&lt;/p&gt;

&lt;p&gt;这是第一层次的思维，我们不妨进一步的想象：如果同时用在多篇新闻上，或者多篇论文上，或者多篇研报上，我们能看到些什么？&lt;/p&gt;

&lt;p&gt;用在特定事件的多篇新闻报道上，我们能更完整的看到事件的全貌。&lt;/p&gt;

&lt;p&gt;用在特定领域的论文上，我们能看到这个领域的分支、进展。&lt;/p&gt;

&lt;p&gt;用在股市研报上，我们能看到市场对特定股票的态度。&lt;/p&gt;

&lt;h1 id=&quot;2任务拆解&quot;&gt;2、任务拆解&lt;/h1&gt;

&lt;p&gt;从数据源来看，我们发现自动文本摘要可以分为两大块方向：单文档摘要和多文档摘要。&lt;/p&gt;

&lt;p&gt;可能有些同学会想，多文档的情况下，直接拼在一起然后用单文档摘要的技术去处理不就可以了吗？确实这是一种思路，但这种思路存在的问题是，虽然有着相似的主题，但不同文档的侧重点不同，而如果直接当做一篇文档来处理，可能会去掉一些相对小众的主题信息。&lt;/p&gt;

&lt;p&gt;从结果来看，自动摘要还能分为两大块方向：抽取式(extraction)和生成式(abstraction)。&lt;/p&gt;

&lt;p&gt;抽取式指的是从原始文本中抽取关键的词句，来组成一篇摘要；而生成式则是从原始文本中通过一些较为高级、复杂的技术，生成出一篇摘要来，会出现一些原文中完全没出现过的词句。&lt;/p&gt;

&lt;p&gt;简单思考后，我们可以发现，抽取式可能更适用于长文本，因为长文本中主题信息分布得比较散，也能有比较多的句子可以用来抽取。然而当面对短文本时，比如两三百词的文本，要做成一两句话的摘要，抽取式可能就很难有较好的效果了，因为这两三百词可能是由八九个句子组成的，而这八九个句子每个都有不同的重要信息，简单从中抽取一两个句子，肯定会有信息遗漏。&lt;/p&gt;

&lt;p&gt;不难想到，单文档摘要比多文档摘要简单一些，抽取式比生成式要简单一些。也正因如此，很多人在写文本摘要的算法博客以及工业实用时，都只讲TextRank，因为这是最好实现的抽取式文本摘要，而且后面我们会提到，这个算法连训练集都不需要！&lt;/p&gt;

&lt;h1 id=&quot;3效果评估&quot;&gt;3、效果评估&lt;/h1&gt;

&lt;p&gt;在展开来谈算法之前，有必要先提一下自动摘要的效果评估方式。一方面是没有效果评估就没法谈优化、没法谈项目贡献，在公司中，这样的项目可能根本就无法通过立项；另一方面也是怕同学们在看了后面一大堆算法之后就不想看这块内容了（笑）。&lt;/p&gt;

&lt;p&gt;最简单粗暴的方式当然就是人工评估啦。这没太多可说的，打分呗。一个分不够就多维度综合打分呗。&lt;/p&gt;

&lt;p&gt;但人力成本那么高，自然还是得想办法出一些指标来自动评估了。&lt;/p&gt;

&lt;p&gt;在对自动摘要进行评估时，我们主要是在想两个问题：&lt;/p&gt;

&lt;p&gt;1、对于文档D来说，摘要A是不是足够好？&lt;/p&gt;

&lt;p&gt;足够好意味着：摘要应该包含原文尽可能多的信息，且尽可能短，而且读起来通顺。&lt;/p&gt;

&lt;p&gt;于是我们可以针对信息量给出一个打分，针对长度给出一个打分，针对可读性给出一个打分。假如三个打分都达到了我们所要的水平，就能认为特定摘要足够好。&lt;/p&gt;

&lt;p&gt;2、同样基于文档D，摘要A是不是比摘要B好？&lt;/p&gt;

&lt;p&gt;我们拿到了信息量、长度、可读性三个打分，然后在判断对两篇摘要孰优孰劣的时候，需要将这些打分通过某种方式整合成一个打分，才能做比较。&lt;/p&gt;

&lt;p&gt;这就意味着，我们需要在信息量、长度、可读性之间做一些权衡，也就是计算机学科里常说的 tradeoff 。&lt;/p&gt;

&lt;p&gt;如何基于信息量打分、如何基于长度打分、如何基于可读性打分、如何权衡三个打分，对于这些问题给出不同解决方案，就是不同评估方式之间的本质区别。&lt;/p&gt;

&lt;h2 id=&quot;rouge&quot;&gt;ROUGE&lt;/h2&gt;

&lt;p&gt;目前比较常用的是ROUGE类的指标，ROUGE=Recall-Oriented Understudy for Gisting Evaluation，字面意思是：用于要点评估的召回导向的替代（原谅我翻译渣…）。&lt;/p&gt;

&lt;p&gt;我们假设有一组参考摘要 $R={r_1,r_2,…,r_m}$ ； $s$ 作为自动生成的摘要； $\Phi_n(d)$ 表示特定文档的n-gram 0-1向量，长度为所有可能的n-gram数量。&lt;/p&gt;

&lt;p&gt;于是我们常见的&lt;strong&gt;ROUGE-N&lt;/strong&gt;是这样定义的：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ROUGE_n(s)=\frac{\sum_{r\in R}(\Phi_n(r)*\Phi_n(s))}{\sum_{r\in R}(\Phi_n(r)*\Phi_n(r))}&lt;/script&gt;

&lt;p&gt;简单来说，就是(重叠的N-gram数)/(参考摘要中的N-gram数)。好了，熟悉机器学习的同学们请坐下，这确实就是Recall。&lt;/p&gt;

&lt;p&gt;ROUGE-N是基于N-grams来计算的，后来有人想出来用最长共同子串（LCS），同时改Recall为F-measure，这就是&lt;strong&gt;ROUGE-L&lt;/strong&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ROUGE_L(s)=\frac{(1+\beta^2)R_{LCS}P_{LCS}}{R_{LCS}+\beta^2P_{LCS}}&lt;/script&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{LCS}(s)=\frac{\sum LCS(r_i, s)}{\sum |r_i|}; P_{LCS}(s)=\frac{\sum LCS(r_i,s)}{|s|}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;ROUGE-W&lt;/strong&gt;: 从ROUGE-L改进而来，加了权重。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ROUGE-S&lt;/strong&gt;: Skip-bigram based co-occurrence statistics. Skip-bigram is any pair of words in their sentence order.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ROUGE-SU&lt;/strong&gt;: Skip-bigram plus unigram-based co-occurrence statistics.&lt;/p&gt;

&lt;h2 id=&quot;信息论&quot;&gt;信息论&lt;/h2&gt;

&lt;p&gt;这一类评估方法，主要思想是看两个分布的散度。主要有两种：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KL divergence&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(p^{\theta_A}||p^{\theta_R})=\sum_{i=1}^{m}p_i^{\theta_A}log \frac{p_i^{\theta_A}}{p_i^{\theta_R}}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Jensen-Shannon divergence&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;JS(p^{\theta_A}||p^{\theta_R})=(KL(p^{\theta_A}||r)+KL(p^{\theta_A}||r))/2=H(r)-H(p^{\theta_A})/2-H(p^{\theta_R})/2&lt;/script&gt;

&lt;h1 id=&quot;4算法&quot;&gt;4、算法&lt;/h1&gt;

&lt;p&gt;接下来就是最核心的部分了——算法。&lt;/p&gt;

&lt;p&gt;正如前面所说的，自动摘要主要分抽取式和生成式两种。基于这两种方式，算法难度上有很大的不同。&lt;/p&gt;

&lt;h2 id=&quot;41-抽取式&quot;&gt;4.1 抽取式&lt;/h2&gt;

&lt;p&gt;抽取式自动摘要的核心思路是，对文档中的所有句子打分，最终挑出若干个权重高的句子来组成摘要。&lt;/p&gt;

&lt;p&gt;最简单粗暴（往往也很靠谱）的做法自然是根据人为规则来挑选句子了，这也是1950年代人们刚开始做自动摘要的思路——利用一些简单的统计特征，例如：句子所包含的单词或短语的数量，句子在文档中的位置，句子是否包含重要单词或短语，句子跟文档标题的词语重叠程度等等，来衡量不同句子的重要性，进而组成摘要。&lt;/p&gt;

&lt;p&gt;当然了，随着发展，算法变得越来越“高级”，效果也越来越好。&lt;/p&gt;

&lt;p&gt;抽取式自动摘要存在两个重要的基础技术：&lt;/p&gt;

&lt;p&gt;1、句子的特征向量表示&lt;/p&gt;

&lt;p&gt;常见的有以下几种：&lt;/p&gt;

&lt;p&gt;基于统计的：我们用一个长度跟语料库（假设$D$由若干个句子组成${s_1,s_2,…,s_M}$）相等的向量 $W=[w_1, w_2, …, w_N]$ 来表示，这个向量中的一个值对应语料库中一个词的重要性。这个值可以是简单的0或1，表示该词语是否存在于特定句子中。又或者是更常见的，TFIDF值：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;TFIDF(s,w)=TF(s,w)*IDF(D,w)=(\frac{count(w \in s)}{len(s)})*log(\frac{len(D)}{len(D|w\in D)+1})&lt;/script&gt;

&lt;p&gt;TF表示词语在该句子中出现的频率；IDF则是通过看该词语是否在很多句子里都存在来评估词语是否“过于常见”。&lt;/p&gt;

&lt;p&gt;基于潜在语义的：这种出来的特征向量长度不一，基本是可以人为控制的，旨在找到隐藏在词句背后的潜在语义特征，比如LDA，word2vec等等，一般缺少明确的可解释性。&lt;/p&gt;

&lt;p&gt;LDA是在构建了TFIDF矩阵后，运用SVD将其拆分成是三个子矩阵，并将其中两个子矩阵提出来相乘得到句子-主题的一个向量。&lt;/p&gt;

&lt;p&gt;word2vec则是通过神经网络进行学习而得。&lt;/p&gt;

&lt;p&gt;当然了，你也可以把上面几种特征向量都拼在一起来用。&lt;/p&gt;

&lt;p&gt;2、句子间的相似度计算&lt;/p&gt;

&lt;p&gt;在有了两个句子的特征向量后，我们需要基于这两个向量来做相似度计算。&lt;/p&gt;

&lt;p&gt;最常用的自然是余弦相似度了：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;CosSim(s_1,s_2)=\frac{s_1*s_2}{|s_1|*|s_2|}&lt;/script&gt;

&lt;p&gt;也就是计算这两个向量间角度的余弦值。&lt;/p&gt;

&lt;p&gt;偶尔可能还会用到Jaccard系数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(v_i,v_j)=\frac{\sum_k min(v_{ik}, v_{jk})}{\sum_kmax(v_{ik},v_{jk})}&lt;/script&gt;

&lt;p&gt;再其他的就很少见了。&lt;/p&gt;

&lt;h3 id=&quot;411-图&quot;&gt;4.1.1 图&lt;/h3&gt;

&lt;p&gt;大名鼎鼎的TextRank和LexRank就是这一类算法。主要都是参考了PageRank的思想。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TextRank&lt;/strong&gt;的好处太多：易于实现、不需要训练集、效果还不错，总而言之，特别适合伸手党。&lt;/p&gt;

&lt;p&gt;就PageRank具体而言，我们先构造一个图 $G={V,E}​$ 。把网页作为一个点，而如果 $V_j​$ 后跟随着 $V_i ​$ ，我们则认为存在一条 $V_j =&amp;gt; V_i​$ 的边。先对图中所有的点赋初始值，然后按照以下公式持续迭代：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;WS(V_i)=(1-d)+d*\sum_{V_j\in IN(V_i)} \frac{w_{ji}}{\sum_{V_k \in OUT(V_j)} w_{jk}}*WS(V_j)&lt;/script&gt;

&lt;p&gt;$d$ 称为阻尼系数，一般取0.85。从迭代的公式中，可以看到，对于特定一个点，分值取决于指向它的其它点的分值，该公式既考虑了这个点有多少入边，还考虑了指向它的点本身有多少出边。&lt;/p&gt;

&lt;p&gt;TextRank基本就是把PageRank直接搬了过来：将句子作为点，取所有的点构造全连接图，两个点之间的边的权重则用句子间相似度来计算。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LexRank&lt;/strong&gt;与TextRank基本相似。（好吧，暂时查不到这两者的区别）&lt;/p&gt;

&lt;h3 id=&quot;412-聚类&quot;&gt;4.1.2 聚类&lt;/h3&gt;

&lt;p&gt;这个方向的思路是：在获取了句子的特征向量之后进行聚类，从而把文章能分成几块主题。最后只要从每个主题里面提取一些句子出来，组成摘要就可以了。&lt;/p&gt;

&lt;p&gt;聚类的通用算法自然可以往上套了，比如K-Means，OBSCAN等等。&lt;/p&gt;

&lt;h3 id=&quot;413-分类&quot;&gt;4.1.3 分类&lt;/h3&gt;

&lt;p&gt;另外一种思路是通过训练一个分类模型来判断特定句子是否应该放在摘要中。既然转成了二分类问题，那所有分类模型都能往上面套了。&lt;/p&gt;

&lt;p&gt;比如贝叶斯、决策树、SVM、HMM、CRF、神经网络等等。&lt;/p&gt;

&lt;h3 id=&quot;414-深层次自然语言分析&quot;&gt;4.1.4 深层次自然语言分析&lt;/h3&gt;

&lt;p&gt;除了机器学习，有一些人在尝试着通过一些较复杂的自然语言分析处理技术来进行文本摘要。总的来说，人们尝试对文本进行解构。&lt;/p&gt;

&lt;p&gt;整体的流程是：分割文档、识别词汇链、通过强词汇链来找到重要语句。&lt;/p&gt;

&lt;h3 id=&quot;415-基于群体智能&quot;&gt;4.1.5 基于群体智能&lt;/h3&gt;

&lt;p&gt;太高级了，没看懂……&lt;/p&gt;

&lt;p&gt;particle swarm optimization&lt;/p&gt;

&lt;p&gt;cuckoo optimization&lt;/p&gt;

&lt;p&gt;bacterial foraging optimization&lt;/p&gt;

&lt;h2 id=&quot;42-生成式&quot;&gt;4.2 生成式&lt;/h2&gt;

&lt;p&gt;生成式文本摘要 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/30559757&quot;&gt;https://zhuanlan.zhihu.com/p/30559757&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1、基于RNN&lt;/p&gt;

&lt;p&gt;A Deep Reinforced Model for Abstractive Summarization&lt;/p&gt;

&lt;p&gt;目前最好的基于RNN的Seq2Seq生成式文本摘要模型之一来自Salesforce，在基本的模型架构上，使用了注意力机制（attention mechanism）和强化学习（reinforcement learning）。&lt;/p&gt;

&lt;p&gt;2、基于CNN&lt;/p&gt;

&lt;p&gt;Convolutional Sequence to Sequence Learning&lt;/p&gt;

&lt;p&gt;基于卷积神经网络的自动文本摘要模型中最具代表性的是由Facebook提出的ConvS2S模型，它的编码器和解码器都由CNN实现，同时也加入了注意力机制&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.ilibrary.me/2017/05/15/sumy-textsum%E5%92%8Cfairsqe&quot;&gt;http://blog.ilibrary.me/2017/05/15/sumy-textsum%E5%92%8Cfairsqe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;sumy &lt;a href=&quot;https://github.com/miso-belica/sumy&quot;&gt;https://github.com/miso-belica/sumy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;tensorflow-textsum &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/textsum&quot;&gt;https://github.com/tensorflow/models/tree/master/research/textsum&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;5中文文本&quot;&gt;5、中文文本&lt;/h1&gt;

&lt;p&gt;关于中文自然语言处理，&lt;a href=&quot;https://github.com/hankcs/HanLP&quot;&gt;HanLP&lt;/a&gt; 是个相当全面的工具，但这是用Java开发的，虽然有提供Python接口（&lt;a href=&quot;https://pypi.org/project/pyhanlp/&quot;&gt;pyhanlp&lt;/a&gt;），但不好改动（太久没用Java，懒得重温…），所以后来我主要还是用 &lt;a href=&quot;https://github.com/letiantian/TextRank4ZH&quot;&gt;TextRank4ZH&lt;/a&gt; ，粗略来看效果也略好一些。&lt;/p&gt;

&lt;h1 id=&quot;6结语&quot;&gt;6、结语&lt;/h1&gt;

&lt;p&gt;通过这段时间的了解，发现这块领域还是很有意思的，但现成的工具比较少，主要也就TextRank了。&lt;/p&gt;

&lt;p&gt;就个人而言，对生成式自动摘要的兴趣更强一些，因为目前项目涉及到的主要是短文本。&lt;/p&gt;

&lt;h1 id=&quot;7引用&quot;&gt;7、引用&lt;/h1&gt;

&lt;p&gt;综述：&lt;/p&gt;

&lt;p&gt;Text Summarization Techniques: A Brief Survey&lt;/p&gt;

&lt;p&gt;A survey on Automatic Text Summarization&lt;/p&gt;

&lt;p&gt;A Survey on Automatic Text Summarization, Dipanjan Das&lt;/p&gt;

&lt;p&gt;A survey on Automatic Text Summarization, N.Nazari&lt;/p&gt;

&lt;p&gt;生成式自动摘要：&lt;/p&gt;

&lt;p&gt;A Deep Reinforced Model for Abstractive Summarization&lt;/p&gt;

&lt;p&gt;Convolutional Sequence to Sequence Learning&lt;/p&gt;</content><author><name></name></author><summary type="html">最近因为项目需要，加上个人兴趣，就找了几篇有关自动文本摘要的综述论文，简单做了些调研、学习，这里做个小结。</summary></entry><entry><title type="html">（TODO） Bias-Variance Tradeoff</title><link href="http://localhost:4034/esl/2018/06/11/bias-variance-tradeoff.html" rel="alternate" type="text/html" title="（TODO） Bias-Variance Tradeoff" /><published>2018-06-11T11:15:00+08:00</published><updated>2018-06-11T11:15:00+08:00</updated><id>http://localhost:4034/esl/2018/06/11/bias-variance-tradeoff</id><content type="html" xml:base="http://localhost:4034/esl/2018/06/11/bias-variance-tradeoff.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;周末仔细研读了ESL的第二章，最大的收获就在于对Bias-Variance有了更清晰的了解。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">LeetCode #22</title><link href="http://localhost:4034/leetcode/2018/06/02/leetcode-22.html" rel="alternate" type="text/html" title="LeetCode #22" /><published>2018-06-02T22:30:00+08:00</published><updated>2018-06-02T22:30:00+08:00</updated><id>http://localhost:4034/leetcode/2018/06/02/leetcode-22</id><content type="html" xml:base="http://localhost:4034/leetcode/2018/06/02/leetcode-22.html">&lt;p&gt;最近重新开始了在刷题，也尽量多写一写吧。&lt;/p&gt;

&lt;h1 id=&quot;一题目大意&quot;&gt;一、题目大意&lt;/h1&gt;

&lt;p&gt;指定有N对括号，需要输出所有可能的、形式上正确的括号组合情况。例如N=3时，有&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;[
  “((()))”,
  “(()())”,
  “(())()”,
  “()(())”,
  “()()()”
]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这五种可能的排列。&lt;/p&gt;

&lt;h1 id=&quot;二解题思路&quot;&gt;二、解题思路：&lt;/h1&gt;

&lt;p&gt;主要参考两篇文章：&lt;/p&gt;

&lt;p&gt;https://leetcode.com/problems/generate-parentheses/solution/&lt;/p&gt;

&lt;p&gt;https://blog.csdn.net/runningtortoises/article/details/45625363&lt;/p&gt;

&lt;p&gt;分别有这么几种思路：&lt;/p&gt;

&lt;p&gt;1、暴力破解&lt;/p&gt;

&lt;p&gt;把所有可能的串都输出来，然后逐个判断合法性。&lt;/p&gt;

&lt;p&gt;2、回溯&lt;/p&gt;

&lt;p&gt;3、Closure Number&lt;/p&gt;

&lt;p&gt;这种思路是我最为喜欢的一种。我们可以发现，一个合法的串总是可以递归的表示为：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;子串1&lt;strong&gt;)&lt;/strong&gt;子串2&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;两个合法的子串以及一对括号。&lt;/p&gt;

&lt;p&gt;想到这一点之后，其实后面的思路很自然的也能想通了：
两个子串的括号对数为N-1，我们只需要遍历所有可能的组合情况就行了，从(0, N-1), (1, N-2), …, (N-1, 0)。
这种思路实际上是按顺序生成 0, 1, …, N 的所有解。&lt;/p&gt;

&lt;p&gt;4、增量&lt;/p&gt;

&lt;p&gt;不断的判断左右括号的数量，进而选择添加左括号还是添加右括号。&lt;/p&gt;

&lt;h1 id=&quot;三具体实现&quot;&gt;三、具体实现：&lt;/h1&gt;

&lt;p&gt;这里就只贴第二种思路的吧。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    递归版
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'({}){}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    非递归版
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        :type n: int
        :rtype: List[str]
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;tmp_res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left_count&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;tmp_res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'({}){}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp_res&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;四、其他&lt;/p&gt;

&lt;p&gt;python的好处在于，代码可以相当精简。同样的算法在c里面去实现的话，难度往往会大很多。
python用久了之后，会发现自己“变笨了”，差不多连c都不会用了。
所以偶尔还是得多练练手的。&lt;/p&gt;</content><author><name></name></author><summary type="html">最近重新开始了在刷题，也尽量多写一写吧。</summary></entry><entry><title type="html">TODO Exercises of ESL Chap.2</title><link href="http://localhost:4034/esl/2018/05/31/esl-2.html" rel="alternate" type="text/html" title="TODO Exercises of ESL Chap.2" /><published>2018-05-31T23:02:00+08:00</published><updated>2018-05-31T23:02:00+08:00</updated><id>http://localhost:4034/esl/2018/05/31/esl-2</id><content type="html" xml:base="http://localhost:4034/esl/2018/05/31/esl-2.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;近期重新拿起了经典的 “The Elements of Statistical Learning” 来好好学习。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">TODO 实时标签</title><link href="http://localhost:4034/work/2018/05/31/realtime-tag.html" rel="alternate" type="text/html" title="TODO 实时标签" /><published>2018-05-31T23:01:00+08:00</published><updated>2018-05-31T23:01:00+08:00</updated><id>http://localhost:4034/work/2018/05/31/realtime-tag</id><content type="html" xml:base="http://localhost:4034/work/2018/05/31/realtime-tag.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;最近新接手的任务——用户实时标签生成。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">TODO 流量预估</title><link href="http://localhost:4034/work/2018/05/31/impose-predict.html" rel="alternate" type="text/html" title="TODO 流量预估" /><published>2018-05-31T23:00:00+08:00</published><updated>2018-05-31T23:00:00+08:00</updated><id>http://localhost:4034/work/2018/05/31/impose-predict</id><content type="html" xml:base="http://localhost:4034/work/2018/05/31/impose-predict.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;最近在做的一个小需求——广告流量预估。这个需求给我带来的收获更多的是技术以外的经验。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">序</title><link href="http://localhost:4034/others/2018/05/26/my-first-post.html" rel="alternate" type="text/html" title="序" /><published>2018-05-26T17:16:00+08:00</published><updated>2018-05-26T17:16:00+08:00</updated><id>http://localhost:4034/others/2018/05/26/my-first-post</id><content type="html" xml:base="http://localhost:4034/others/2018/05/26/my-first-post.html">&lt;p&gt;寻思写博客这事，也不是一天两天了。&lt;/p&gt;

&lt;p&gt;和大家一样，我知道写博客有各种好处；和绝大多数人一样，我也没能累积下来多少篇。
虽然不知道这次能坚持多久，但能写一篇算一篇吧。&lt;/p&gt;</content><author><name></name></author><summary type="html">寻思写博客这事，也不是一天两天了。</summary></entry></feed>