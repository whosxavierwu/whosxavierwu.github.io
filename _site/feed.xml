<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.2">Jekyll</generator><link href="http://localhost:8080/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:8080/" rel="alternate" type="text/html" /><updated>2019-03-16T17:37:31+08:00</updated><id>http://localhost:8080/</id><title type="html">whosxavierwu’s blog</title><subtitle>Keep learning, deep learning. </subtitle><entry><title type="html">自动文本摘要</title><link href="http://localhost:8080/text-summarization/2019/03/15/automatic-text-summarization.html" rel="alternate" type="text/html" title="自动文本摘要" /><published>2019-03-15T17:00:00+08:00</published><updated>2019-03-15T17:00:00+08:00</updated><id>http://localhost:8080/text-summarization/2019/03/15/automatic-text-summarization</id><content type="html" xml:base="http://localhost:8080/text-summarization/2019/03/15/automatic-text-summarization.html">&lt;p&gt;最近因为项目需要，加上个人兴趣，就找了几篇有关自动文本摘要的综述论文，简单做了些调研、学习，这里做个小结。&lt;/p&gt;

&lt;h1 id=&quot;0概述&quot;&gt;0、概述&lt;/h1&gt;

&lt;p&gt;自动文本摘要，顾名思义，就是自动的从文本中生成摘要。&lt;/p&gt;

&lt;p&gt;其实这方面的研究最早在1950年代就开始了，但一直得不到大众的关注，在国际上尚且如此，在国内的情况就更加尴尬了。基于中文文本的自动摘要，在国内能找到的资料、工具并不多。&lt;/p&gt;

&lt;h1 id=&quot;1应用场景&quot;&gt;1、应用场景&lt;/h1&gt;

&lt;p&gt;自动文本摘要能用在哪些地方呢？从个人习惯来说，我倾向于把日常接触到的文字内容分为“水货”和“干货”两种。&lt;/p&gt;

&lt;p&gt;“水货”是指哪些呢？就是那些看完之后没有太多实际收获的内容，比如很多公众号里的鸡汤文、鸡血文、狗血文等等，对于很多人而言，小说类侧重故事性的文章，也都是“水货”。总的来说，我们发现，其实自动文本摘要并不适用于这一类内容，因为读者接触这些内容是为了来体验、来感受的，而不是抱着功利心来的。&lt;/p&gt;

&lt;p&gt;自动文本摘要主要还是应用于“干货”上，新闻文章、科学文献、股市研报等等，人们从这些内容中，想要的是新的信息收获。&lt;/p&gt;

&lt;p&gt;那沿着这个思路，我们就可以想到自动摘要的应用场景都有哪些了。&lt;/p&gt;

&lt;p&gt;用在新闻文章上，我们可以看到新闻快报、头条，像我几乎每天上班路上都会打开的虎嗅早报、36氪早报，以及微博上的新闻推送，其实是能通过自动摘要来生成的。&lt;/p&gt;

&lt;p&gt;用在科学文献上，我们更快速的看论文，涨知识。&lt;/p&gt;

&lt;p&gt;用在股市研报上，我们看一眼就能知道A股又要跌多少。&lt;/p&gt;

&lt;p&gt;这是第一层次的思维，我们不妨进一步的想象：如果同时用在多篇新闻上，或者多篇论文上，或者多篇研报上，我们能看到些什么？&lt;/p&gt;

&lt;p&gt;用在特定事件的多篇新闻报道上，我们能更完整的看到事件的全貌。&lt;/p&gt;

&lt;p&gt;用在特定领域的论文上，我们能看到这个领域的分支、进展。&lt;/p&gt;

&lt;p&gt;用在股市研报上，我们能看到市场对特定股票的态度。&lt;/p&gt;

&lt;h1 id=&quot;2任务拆解&quot;&gt;2、任务拆解&lt;/h1&gt;

&lt;p&gt;从数据源来看，我们发现自动文本摘要可以分为两大块方向：单文档摘要和多文档摘要。&lt;/p&gt;

&lt;p&gt;可能有些同学会想，多文档的情况下，直接拼在一起然后用单文档摘要的技术去处理不就可以了吗？确实这是一种思路，但这种思路存在的问题是，虽然有着相似的主题，但不同文档的侧重点不同，而如果直接当做一篇文档来处理，可能会去掉一些相对小众的主题信息。&lt;/p&gt;

&lt;p&gt;从结果来看，自动摘要还能分为两大块方向：抽取式(extraction)和生成式(abstraction)。&lt;/p&gt;

&lt;p&gt;抽取式指的是从原始文本中抽取关键的词句，来组成一篇摘要；而生成式则是从原始文本中通过一些较为高级、复杂的技术，生成出一篇摘要来，会出现一些原文中完全没出现过的词句。&lt;/p&gt;

&lt;p&gt;简单思考后，我们可以发现，抽取式可能更适用于长文本，因为长文本中主题信息分布得比较散，也能有比较多的句子可以用来抽取。然而当面对短文本时，比如两三百词的文本，要做成一两句话的摘要，抽取式可能就很难有较好的效果了，因为这两三百词可能是由八九个句子组成的，而这八九个句子每个都有不同的重要信息，简单从中抽取一两个句子，肯定会有信息遗漏。&lt;/p&gt;

&lt;p&gt;不难想到，单文档摘要比多文档摘要简单一些，抽取式比生成式要简单一些。也正因如此，很多人在写文本摘要的算法博客以及工业实用时，都只讲TextRank，因为这是最好实现的抽取式文本摘要，而且后面我们会提到，这个算法连训练集都不需要！&lt;/p&gt;

&lt;h1 id=&quot;3效果评估&quot;&gt;3、效果评估&lt;/h1&gt;

&lt;p&gt;在展开来谈算法之前，有必要先提一下自动摘要的效果评估方式。一方面是没有效果评估就没法谈优化、没法谈项目贡献，在公司中，这样的项目可能根本就无法通过立项；另一方面也是怕同学们在看了后面一大堆算法之后就不想看这块内容了（笑）。&lt;/p&gt;

&lt;p&gt;最简单粗暴的方式当然就是人工评估啦。这没太多可说的，打分呗。一个分不够就多维度综合打分呗。&lt;/p&gt;

&lt;p&gt;但人力成本那么高，自然还是得想办法出一些指标来自动评估了。&lt;/p&gt;

&lt;p&gt;在对自动摘要进行评估时，我们主要是在想两个问题：&lt;/p&gt;

&lt;p&gt;1、对于文档D来说，摘要A是不是足够好？&lt;/p&gt;

&lt;p&gt;足够好意味着：摘要应该包含原文尽可能多的信息，且尽可能短，而且读起来通顺。&lt;/p&gt;

&lt;p&gt;于是我们可以针对信息量给出一个打分，针对长度给出一个打分，针对可读性给出一个打分。假如三个打分都达到了我们所要的水平，就能认为特定摘要足够好。&lt;/p&gt;

&lt;p&gt;2、同样基于文档D，摘要A是不是比摘要B好？&lt;/p&gt;

&lt;p&gt;我们拿到了信息量、长度、可读性三个打分，然后在判断对两篇摘要孰优孰劣的时候，需要将这些打分通过某种方式整合成一个打分，才能做比较。&lt;/p&gt;

&lt;p&gt;这就意味着，我们需要在信息量、长度、可读性之间做一些权衡，也就是计算机学科里常说的 tradeoff 。&lt;/p&gt;

&lt;p&gt;如何基于信息量打分、如何基于长度打分、如何基于可读性打分、如何权衡三个打分，对于这些问题给出不同解决方案，就是不同评估方式之间的本质区别。&lt;/p&gt;

&lt;h2 id=&quot;rouge&quot;&gt;ROUGE&lt;/h2&gt;

&lt;p&gt;目前比较常用的是ROUGE类的指标，ROUGE=Recall-Oriented Understudy for Gisting Evaluation，字面意思是：用于要点评估的召回导向的替代（原谅我翻译渣…）。&lt;/p&gt;

&lt;p&gt;我们假设有一组参考摘要 $R={r_1,r_2,…,r_m}$ ； $s$ 作为自动生成的摘要； $\Phi_n(d)$ 表示特定文档的n-gram 0-1向量，长度为所有可能的n-gram数量。&lt;/p&gt;

&lt;p&gt;于是我们常见的&lt;strong&gt;ROUGE-N&lt;/strong&gt;是这样定义的：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ROUGE_n(s)=\frac{\sum_{r\in R}(\Phi_n(r)*\Phi_n(s))}{\sum_{r\in R}(\Phi_n(r)*\Phi_n(r))}&lt;/script&gt;

&lt;p&gt;简单来说，就是(重叠的N-gram数)/(参考摘要中的N-gram数)。好了，熟悉机器学习的同学们请坐下，这确实就是Recall。&lt;/p&gt;

&lt;p&gt;ROUGE-N是基于N-grams来计算的，后来有人想出来用最长共同子串（LCS），同时改Recall为F-measure，这就是&lt;strong&gt;ROUGE-L&lt;/strong&gt;：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;ROUGE_L(s)=\frac{(1+\beta^2)R_{LCS}P_{LCS}}{R_{LCS}+\beta^2P_{LCS}}&lt;/script&gt;

&lt;p&gt;其中：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;R_{LCS}(s)=\frac{\sum LCS(r_i, s)}{\sum |r_i|}; P_{LCS}(s)=\frac{\sum LCS(r_i,s)}{|s|}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;ROUGE-W&lt;/strong&gt;: 从ROUGE-L改进而来，加了权重。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ROUGE-S&lt;/strong&gt;: Skip-bigram based co-occurrence statistics. Skip-bigram is any pair of words in their sentence order.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ROUGE-SU&lt;/strong&gt;: Skip-bigram plus unigram-based co-occurrence statistics.&lt;/p&gt;

&lt;h2 id=&quot;信息论&quot;&gt;信息论&lt;/h2&gt;

&lt;p&gt;这一类评估方法，主要思想是看两个分布的散度。主要有两种：&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KL divergence&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;KL(p^{\theta_A}||p^{\theta_R})=\sum_{i=1}^{m}p_i^{\theta_A}log \frac{p_i^{\theta_A}}{p_i^{\theta_R}}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Jensen-Shannon divergence&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;JS(p^{\theta_A}||p^{\theta_R})=(KL(p^{\theta_A}||r)+KL(p^{\theta_A}||r))/2=H(r)-H(p^{\theta_A})/2-H(p^{\theta_R})/2&lt;/script&gt;

&lt;h1 id=&quot;4算法&quot;&gt;4、算法&lt;/h1&gt;

&lt;p&gt;接下来就是最核心的部分了——算法。&lt;/p&gt;

&lt;p&gt;正如前面所说的，自动摘要主要分抽取式和生成式两种。基于这两种方式，算法难度上有很大的不同。&lt;/p&gt;

&lt;h2 id=&quot;41-抽取式&quot;&gt;4.1 抽取式&lt;/h2&gt;

&lt;p&gt;抽取式自动摘要的核心思路是，对文档中的所有句子打分，最终挑出若干个权重高的句子来组成摘要。&lt;/p&gt;

&lt;p&gt;最简单粗暴（往往也很靠谱）的做法自然是根据人为规则来挑选句子了，这也是1950年代人们刚开始做自动摘要的思路——利用一些简单的统计特征，例如：句子所包含的单词或短语的数量，句子在文档中的位置，句子是否包含重要单词或短语，句子跟文档标题的词语重叠程度等等，来衡量不同句子的重要性，进而组成摘要。&lt;/p&gt;

&lt;p&gt;当然了，随着发展，算法变得越来越“高级”，效果也越来越好。&lt;/p&gt;

&lt;p&gt;抽取式自动摘要存在两个重要的基础技术：&lt;/p&gt;

&lt;p&gt;1、句子的特征向量表示&lt;/p&gt;

&lt;p&gt;常见的有以下几种：&lt;/p&gt;

&lt;p&gt;基于统计的：我们用一个长度跟语料库（假设$D$由若干个句子组成${s_1,s_2,…,s_M}$）相等的向量 $W=[w_1, w_2, …, w_N]$ 来表示，这个向量中的一个值对应语料库中一个词的重要性。这个值可以是简单的0或1，表示该词语是否存在于特定句子中。又或者是更常见的，TFIDF值：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;TFIDF(s,w)=TF(s,w)*IDF(D,w)=(\frac{count(w \in s)}{len(s)})*log(\frac{len(D)}{len(D|w\in D)+1})&lt;/script&gt;

&lt;p&gt;TF表示词语在该句子中出现的频率；IDF则是通过看该词语是否在很多句子里都存在来评估词语是否“过于常见”。&lt;/p&gt;

&lt;p&gt;基于潜在语义的：这种出来的特征向量长度不一，基本是可以人为控制的，旨在找到隐藏在词句背后的潜在语义特征，比如LDA，word2vec等等，一般缺少明确的可解释性。&lt;/p&gt;

&lt;p&gt;LDA是在构建了TFIDF矩阵后，运用SVD将其拆分成是三个子矩阵，并将其中两个子矩阵提出来相乘得到句子-主题的一个向量。&lt;/p&gt;

&lt;p&gt;word2vec则是通过神经网络进行学习而得。&lt;/p&gt;

&lt;p&gt;当然了，你也可以把上面几种特征向量都拼在一起来用。&lt;/p&gt;

&lt;p&gt;2、句子间的相似度计算&lt;/p&gt;

&lt;p&gt;在有了两个句子的特征向量后，我们需要基于这两个向量来做相似度计算。&lt;/p&gt;

&lt;p&gt;最常用的自然是余弦相似度了：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;CosSim(s_1,s_2)=\frac{s_1*s_2}{|s_1|*|s_2|}&lt;/script&gt;

&lt;p&gt;也就是计算这两个向量间角度的余弦值。&lt;/p&gt;

&lt;p&gt;偶尔可能还会用到Jaccard系数：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(v_i,v_j)=\frac{\sum_k min(v_{ik}, v_{jk})}{\sum_kmax(v_{ik},v_{jk})}&lt;/script&gt;

&lt;p&gt;再其他的就很少见了。&lt;/p&gt;

&lt;h3 id=&quot;411-图&quot;&gt;4.1.1 图&lt;/h3&gt;

&lt;p&gt;大名鼎鼎的TextRank和LexRank就是这一类算法。主要都是参考了PageRank的思想。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TextRank&lt;/strong&gt;的好处太多：易于实现、不需要训练集、效果还不错，总而言之，特别适合伸手党。&lt;/p&gt;

&lt;p&gt;就PageRank具体而言，我们先构造一个图 $G={V,E}​$ 。把网页作为一个点，而如果 $V_j​$ 后跟随着 $V_i ​$ ，我们则认为存在一条 $V_j =&amp;gt; V_i​$ 的边。先对图中所有的点赋初始值，然后按照以下公式持续迭代：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;WS(V_i)=(1-d)+d*\sum_{V_j\in IN(V_i)} \frac{w_{ji}}{\sum_{V_k \in OUT(V_j)} w_{jk}}*WS(V_j)&lt;/script&gt;

&lt;p&gt;$d$ 称为阻尼系数，一般取0.85。从迭代的公式中，可以看到，对于特定一个点，分值取决于指向它的其它点的分值，该公式既考虑了这个点有多少入边，还考虑了指向它的点本身有多少出边。&lt;/p&gt;

&lt;p&gt;TextRank基本就是把PageRank直接搬了过来：将句子作为点，取所有的点构造全连接图，两个点之间的边的权重则用句子间相似度来计算。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LexRank&lt;/strong&gt;与TextRank基本相似。（好吧，暂时查不到这两者的区别）&lt;/p&gt;

&lt;h3 id=&quot;412-聚类&quot;&gt;4.1.2 聚类&lt;/h3&gt;

&lt;p&gt;这个方向的思路是：在获取了句子的特征向量之后进行聚类，从而把文章能分成几块主题。最后只要从每个主题里面提取一些句子出来，组成摘要就可以了。&lt;/p&gt;

&lt;p&gt;聚类的通用算法自然可以往上套了，比如K-Means，OBSCAN等等。&lt;/p&gt;

&lt;h3 id=&quot;413-分类&quot;&gt;4.1.3 分类&lt;/h3&gt;

&lt;p&gt;另外一种思路是通过训练一个分类模型来判断特定句子是否应该放在摘要中。既然转成了二分类问题，那所有分类模型都能往上面套了。&lt;/p&gt;

&lt;p&gt;比如贝叶斯、决策树、SVM、HMM、CRF、神经网络等等。&lt;/p&gt;

&lt;h3 id=&quot;414-深层次自然语言分析&quot;&gt;4.1.4 深层次自然语言分析&lt;/h3&gt;

&lt;p&gt;除了机器学习，有一些人在尝试着通过一些较复杂的自然语言分析处理技术来进行文本摘要。总的来说，人们尝试对文本进行解构。&lt;/p&gt;

&lt;p&gt;整体的流程是：分割文档、识别词汇链、通过强词汇链来找到重要语句。&lt;/p&gt;

&lt;h3 id=&quot;415-基于群体智能&quot;&gt;4.1.5 基于群体智能&lt;/h3&gt;

&lt;p&gt;太高级了，没看懂……&lt;/p&gt;

&lt;p&gt;particle swarm optimization&lt;/p&gt;

&lt;p&gt;cuckoo optimization&lt;/p&gt;

&lt;p&gt;bacterial foraging optimization&lt;/p&gt;

&lt;h2 id=&quot;42-生成式&quot;&gt;4.2 生成式&lt;/h2&gt;

&lt;p&gt;生成式文本摘要 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/30559757&quot;&gt;https://zhuanlan.zhihu.com/p/30559757&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1、基于RNN&lt;/p&gt;

&lt;p&gt;A Deep Reinforced Model for Abstractive Summarization&lt;/p&gt;

&lt;p&gt;目前最好的基于RNN的Seq2Seq生成式文本摘要模型之一来自Salesforce，在基本的模型架构上，使用了注意力机制（attention mechanism）和强化学习（reinforcement learning）。&lt;/p&gt;

&lt;p&gt;2、基于CNN&lt;/p&gt;

&lt;p&gt;Convolutional Sequence to Sequence Learning&lt;/p&gt;

&lt;p&gt;基于卷积神经网络的自动文本摘要模型中最具代表性的是由Facebook提出的ConvS2S模型，它的编码器和解码器都由CNN实现，同时也加入了注意力机制&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.ilibrary.me/2017/05/15/sumy-textsum%E5%92%8Cfairsqe&quot;&gt;http://blog.ilibrary.me/2017/05/15/sumy-textsum%E5%92%8Cfairsqe&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;sumy &lt;a href=&quot;https://github.com/miso-belica/sumy&quot;&gt;https://github.com/miso-belica/sumy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;tensorflow-textsum &lt;a href=&quot;https://github.com/tensorflow/models/tree/master/research/textsum&quot;&gt;https://github.com/tensorflow/models/tree/master/research/textsum&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;5中文文本&quot;&gt;5、中文文本&lt;/h1&gt;

&lt;p&gt;关于中文自然语言处理，&lt;a href=&quot;https://github.com/hankcs/HanLP&quot;&gt;HanLP&lt;/a&gt; 是个相当全面的工具，但这是用Java开发的，虽然有提供Python接口（&lt;a href=&quot;https://pypi.org/project/pyhanlp/&quot;&gt;pyhanlp&lt;/a&gt;），但不好改动（太久没用Java，懒得重温…），所以后来我主要还是用 &lt;a href=&quot;https://github.com/letiantian/TextRank4ZH&quot;&gt;TextRank4ZH&lt;/a&gt; ，粗略来看效果也略好一些。&lt;/p&gt;

&lt;h1 id=&quot;6结语&quot;&gt;6、结语&lt;/h1&gt;

&lt;p&gt;通过这段时间的了解，发现这块领域还是很有意思的，但现成的工具比较少，主要也就TextRank了。&lt;/p&gt;

&lt;p&gt;就个人而言，对生成式自动摘要的兴趣更强一些，因为目前项目涉及到的主要是短文本。&lt;/p&gt;

&lt;h1 id=&quot;7引用&quot;&gt;7、引用&lt;/h1&gt;

&lt;p&gt;综述：&lt;/p&gt;

&lt;p&gt;Text Summarization Techniques: A Brief Survey&lt;/p&gt;

&lt;p&gt;A survey on Automatic Text Summarization&lt;/p&gt;

&lt;p&gt;A Survey on Automatic Text Summarization, Dipanjan Das&lt;/p&gt;

&lt;p&gt;A survey on Automatic Text Summarization, N.Nazari&lt;/p&gt;

&lt;p&gt;生成式自动摘要：&lt;/p&gt;

&lt;p&gt;A Deep Reinforced Model for Abstractive Summarization&lt;/p&gt;

&lt;p&gt;Convolutional Sequence to Sequence Learning&lt;/p&gt;</content><author><name></name></author><summary type="html">最近因为项目需要，加上个人兴趣，就找了几篇有关自动文本摘要的综述论文，简单做了些调研、学习，这里做个小结。</summary></entry><entry><title type="html">（TODO） Bias-Variance Tradeoff</title><link href="http://localhost:8080/esl/2018/06/11/bias-variance-tradeoff.html" rel="alternate" type="text/html" title="（TODO） Bias-Variance Tradeoff" /><published>2018-06-11T11:15:00+08:00</published><updated>2018-06-11T11:15:00+08:00</updated><id>http://localhost:8080/esl/2018/06/11/bias-variance-tradeoff</id><content type="html" xml:base="http://localhost:8080/esl/2018/06/11/bias-variance-tradeoff.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;周末仔细研读了ESL的第二章，最大的收获就在于对Bias-Variance有了更清晰的了解。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">LeetCode #22</title><link href="http://localhost:8080/leetcode/2018/06/02/leetcode-22.html" rel="alternate" type="text/html" title="LeetCode #22" /><published>2018-06-02T22:30:00+08:00</published><updated>2018-06-02T22:30:00+08:00</updated><id>http://localhost:8080/leetcode/2018/06/02/leetcode-22</id><content type="html" xml:base="http://localhost:8080/leetcode/2018/06/02/leetcode-22.html">&lt;p&gt;最近重新开始了在刷题，也尽量多写一写吧。&lt;/p&gt;

&lt;h1 id=&quot;一题目大意&quot;&gt;一、题目大意&lt;/h1&gt;

&lt;p&gt;指定有N对括号，需要输出所有可能的、形式上正确的括号组合情况。例如N=3时，有&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;[
  “((()))”,
  “(()())”,
  “(())()”,
  “()(())”,
  “()()()”
]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这五种可能的排列。&lt;/p&gt;

&lt;h1 id=&quot;二解题思路&quot;&gt;二、解题思路：&lt;/h1&gt;

&lt;p&gt;主要参考两篇文章：&lt;/p&gt;

&lt;p&gt;https://leetcode.com/problems/generate-parentheses/solution/&lt;/p&gt;

&lt;p&gt;https://blog.csdn.net/runningtortoises/article/details/45625363&lt;/p&gt;

&lt;p&gt;分别有这么几种思路：&lt;/p&gt;

&lt;p&gt;1、暴力破解&lt;/p&gt;

&lt;p&gt;把所有可能的串都输出来，然后逐个判断合法性。&lt;/p&gt;

&lt;p&gt;2、回溯&lt;/p&gt;

&lt;p&gt;3、Closure Number&lt;/p&gt;

&lt;p&gt;这种思路是我最为喜欢的一种。我们可以发现，一个合法的串总是可以递归的表示为：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;(&lt;/strong&gt;子串1&lt;strong&gt;)&lt;/strong&gt;子串2&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;两个合法的子串以及一对括号。&lt;/p&gt;

&lt;p&gt;想到这一点之后，其实后面的思路很自然的也能想通了：
两个子串的括号对数为N-1，我们只需要遍历所有可能的组合情况就行了，从(0, N-1), (1, N-2), …, (N-1, 0)。
这种思路实际上是按顺序生成 0, 1, …, N 的所有解。&lt;/p&gt;

&lt;p&gt;4、增量&lt;/p&gt;

&lt;p&gt;不断的判断左右括号的数量，进而选择添加左括号还是添加右括号。&lt;/p&gt;

&lt;h1 id=&quot;三具体实现&quot;&gt;三、具体实现：&lt;/h1&gt;

&lt;p&gt;这里就只贴第二种思路的吧。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    递归版
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'({}){}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ans&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    非递归版
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateParenthesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        :type n: int
        :rtype: List[str]
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;tmp_res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left_count&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left_count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                        &lt;span class=&quot;n&quot;&gt;tmp_res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'({}){}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp_res&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;四、其他&lt;/p&gt;

&lt;p&gt;python的好处在于，代码可以相当精简。同样的算法在c里面去实现的话，难度往往会大很多。
python用久了之后，会发现自己“变笨了”，差不多连c都不会用了。
所以偶尔还是得多练练手的。&lt;/p&gt;</content><author><name></name></author><summary type="html">最近重新开始了在刷题，也尽量多写一写吧。</summary></entry><entry><title type="html">TODO Exercises of ESL Chap.2</title><link href="http://localhost:8080/esl/2018/05/31/esl-2.html" rel="alternate" type="text/html" title="TODO Exercises of ESL Chap.2" /><published>2018-05-31T23:02:00+08:00</published><updated>2018-05-31T23:02:00+08:00</updated><id>http://localhost:8080/esl/2018/05/31/esl-2</id><content type="html" xml:base="http://localhost:8080/esl/2018/05/31/esl-2.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;近期重新拿起了经典的 “The Elements of Statistical Learning” 来好好学习。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">TODO 实时标签</title><link href="http://localhost:8080/work/2018/05/31/realtime-tag.html" rel="alternate" type="text/html" title="TODO 实时标签" /><published>2018-05-31T23:01:00+08:00</published><updated>2018-05-31T23:01:00+08:00</updated><id>http://localhost:8080/work/2018/05/31/realtime-tag</id><content type="html" xml:base="http://localhost:8080/work/2018/05/31/realtime-tag.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;最近新接手的任务——用户实时标签生成。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">TODO 流量预估</title><link href="http://localhost:8080/work/2018/05/31/impose-predict.html" rel="alternate" type="text/html" title="TODO 流量预估" /><published>2018-05-31T23:00:00+08:00</published><updated>2018-05-31T23:00:00+08:00</updated><id>http://localhost:8080/work/2018/05/31/impose-predict</id><content type="html" xml:base="http://localhost:8080/work/2018/05/31/impose-predict.html">&lt;p&gt;（待填坑）&lt;/p&gt;

&lt;p&gt;最近在做的一个小需求——广告流量预估。这个需求给我带来的收获更多的是技术以外的经验。&lt;/p&gt;</content><author><name></name></author><summary type="html">（待填坑）</summary></entry><entry><title type="html">序</title><link href="http://localhost:8080/others/2018/05/26/my-first-post.html" rel="alternate" type="text/html" title="序" /><published>2018-05-26T17:16:00+08:00</published><updated>2018-05-26T17:16:00+08:00</updated><id>http://localhost:8080/others/2018/05/26/my-first-post</id><content type="html" xml:base="http://localhost:8080/others/2018/05/26/my-first-post.html">&lt;p&gt;寻思写博客这事，也不是一天两天了。&lt;/p&gt;

&lt;p&gt;和大家一样，我知道写博客有各种好处；和绝大多数人一样，我也没能累积下来多少篇。
虽然不知道这次能坚持多久，但能写一篇算一篇吧。&lt;/p&gt;</content><author><name></name></author><summary type="html">寻思写博客这事，也不是一天两天了。</summary></entry><entry><title type="html">聚类算法总结 - Hierarchical Clustering</title><link href="http://localhost:8080/datamining/2015/09/24/hierarchical-clustering.html" rel="alternate" type="text/html" title="聚类算法总结 - Hierarchical Clustering" /><published>2015-09-24T16:22:00+08:00</published><updated>2015-09-24T16:22:00+08:00</updated><id>http://localhost:8080/datamining/2015/09/24/hierarchical-clustering</id><content type="html" xml:base="http://localhost:8080/datamining/2015/09/24/hierarchical-clustering.html">&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;算法&lt;/th&gt;
      &lt;th&gt;概括&lt;/th&gt;
      &lt;th&gt;优缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;AGNES&lt;/td&gt;
      &lt;td&gt;典型的凝聚式层次聚类&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DIANA&lt;/td&gt;
      &lt;td&gt;典型的划分式层次聚类&lt;/td&gt;
      &lt;td&gt;划分式层次聚类的复杂度比凝聚式的大得多，所以较为少用。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CURE&lt;/td&gt;
      &lt;td&gt;用到了kd-tree跟heap。&lt;br /&gt;合并两个类的时候，先选若干well-scattered的点。从中挑出离中心最远的点，之后再挑离该点最远的点…如此得到一堆代表点，基于这些点去做层次聚类。&lt;br /&gt;&lt;br /&gt;对于大数据：先随机抽样，再对样本进行分区，然后对每个分区局部聚类，最后对局部聚类进行全局聚类。&lt;/td&gt;
      &lt;td&gt;时间上最坏是：$O(n^2log(n))$&lt;br /&gt;若数据维度较小，可以降到：$O(n^2)$&lt;br /&gt;空间复杂度是：$O(n)$&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ROCK&lt;/td&gt;
      &lt;td&gt;1.生成相似度矩阵。&lt;br /&gt;2.根据相似度阈值得到邻居矩阵-A。&lt;br /&gt;3.计算链接矩阵-L=A x A &lt;br /&gt;4.计算相似性的度量（Goodness Measure），将相似性最高的两个对象合并。（用到了链接矩阵）&lt;br /&gt;&lt;br /&gt;ROCK算法首先用相似度阀值和共同邻居的概念，从给定的数据相似度矩阵中构建一个稀疏图，然后对该稀疏图使用分层聚类算法进行聚类&lt;/td&gt;
      &lt;td&gt;CURE算法不能处理枚举型数据，而ROCK算法是在CURE基础之上适用于枚举数据的聚结分层聚类算法。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Chameleon&lt;/td&gt;
      &lt;td&gt;1.由数据集构造成一个K-近邻图$G_k$&lt;br /&gt;2.通过图的划分算法将图$G_k$划分成大量的子图，每个子图代表一个初始子簇&lt;br /&gt;3.凝聚式层次聚类&lt;/td&gt;
      &lt;td&gt;Chameleon跟CURE和DBSCAN相比，在发现高质量的任意形状的聚类方面有更强的能力。但是，在最坏的情况下，高维数据的处理代价可能对n个对象需要$O(n^2)$的时间。&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;BIRCH&lt;/td&gt;
      &lt;td&gt;用到了$CF&amp;lt;n, LS, SS&amp;gt;$&lt;br /&gt;CF-tree类似于B-树，有两个参数：内部节点平衡因子$B$，叶节点平衡因子$L$，簇半径阈值$T$。&lt;br /&gt;&lt;br /&gt;1.自上而下选择最近的子节点&lt;br /&gt;2.到达子节点后，检查最近的元组$CF_i$能否吸收此数据点&lt;br /&gt;若能吸收，则更新CF值&lt;br /&gt;否则考虑是否可以添加一个新的元组&lt;br /&gt;如果可以，则添加一个新的元组&lt;br /&gt;否则，分裂最远的一对元组，作为种子，按最近距离重新分配其它元组&lt;br /&gt;3.更新每个非叶节点的CF信息，如果分裂节点，在父节点中插入新的元组，检查分裂，直到root&lt;/td&gt;
      &lt;td&gt;BIRCH优点：&lt;br /&gt;1.节省内存。叶子节点放在磁盘分区&lt;br /&gt;2. 在对树进行插入或查找操作很快。&lt;br /&gt;3.一遍扫描数据库即可建树。&lt;br /&gt;4.可识别噪声点。&lt;br /&gt;5. 可作为其他聚类算法的预处理过程&lt;br /&gt;&lt;br /&gt;BIRCH缺点：&lt;br /&gt;1.结果依赖于数据点的插入顺序。&lt;br /&gt;2.对非球状的簇聚类效果不好。&lt;br /&gt;3.对高维数据聚类效果不好。&lt;br /&gt;4.最后得出来的簇可能和自然簇相差很大。&lt;br /&gt;5.在整个过程中算法一旦中断，一切必须从头再来。&lt;br /&gt;6.局部性&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;*BUBBLE&lt;/td&gt;
      &lt;td&gt;把BIRCH算法的中心和半径概念推广到普通的距离空间&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;*BUBBLE-FM&lt;/td&gt;
      &lt;td&gt;通过减少距离的计算次数，提高了BUBBLE算法的效率&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Probabilistic agglomerative clustering&lt;/td&gt;
      &lt;td&gt;距离度量用：&lt;br /&gt;&lt;script type=&quot;math/tex&quot;&gt;dist(C_1,C_2 )=-log ((P(C_1∪C_2))/(P(C_1)P(C_2)) )&lt;/script&gt; &lt;br /&gt;如果dist小于零，则合并两个簇。&lt;/td&gt;
      &lt;td&gt;易于理解&lt;br /&gt;一般跟其他凝聚式层次聚类算法的效率差不多&lt;br /&gt;但是：it outputs only one hierarchy with respect to a chosen probabilistic model; it cannot handle the uncertainty of cluster hierarchies.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name></name></author><summary type="html">算法 概括 优缺点 AGNES 典型的凝聚式层次聚类   DIANA 典型的划分式层次聚类 划分式层次聚类的复杂度比凝聚式的大得多，所以较为少用。 CURE 用到了kd-tree跟heap。合并两个类的时候，先选若干well-scattered的点。从中挑出离中心最远的点，之后再挑离该点最远的点…如此得到一堆代表点，基于这些点去做层次聚类。对于大数据：先随机抽样，再对样本进行分区，然后对每个分区局部聚类，最后对局部聚类进行全局聚类。 时间上最坏是：$O(n^2log(n))$若数据维度较小，可以降到：$O(n^2)$空间复杂度是：$O(n)$ ROCK 1.生成相似度矩阵。2.根据相似度阈值得到邻居矩阵-A。3.计算链接矩阵-L=A x A 4.计算相似性的度量（Goodness Measure），将相似性最高的两个对象合并。（用到了链接矩阵）ROCK算法首先用相似度阀值和共同邻居的概念，从给定的数据相似度矩阵中构建一个稀疏图，然后对该稀疏图使用分层聚类算法进行聚类 CURE算法不能处理枚举型数据，而ROCK算法是在CURE基础之上适用于枚举数据的聚结分层聚类算法。 Chameleon 1.由数据集构造成一个K-近邻图$G_k$2.通过图的划分算法将图$G_k$划分成大量的子图，每个子图代表一个初始子簇3.凝聚式层次聚类 Chameleon跟CURE和DBSCAN相比，在发现高质量的任意形状的聚类方面有更强的能力。但是，在最坏的情况下，高维数据的处理代价可能对n个对象需要$O(n^2)$的时间。 BIRCH 用到了$CF&amp;lt;n, LS, SS&amp;gt;$CF-tree类似于B-树，有两个参数：内部节点平衡因子$B$，叶节点平衡因子$L$，簇半径阈值$T$。1.自上而下选择最近的子节点2.到达子节点后，检查最近的元组$CF_i$能否吸收此数据点若能吸收，则更新CF值否则考虑是否可以添加一个新的元组如果可以，则添加一个新的元组否则，分裂最远的一对元组，作为种子，按最近距离重新分配其它元组3.更新每个非叶节点的CF信息，如果分裂节点，在父节点中插入新的元组，检查分裂，直到root BIRCH优点：1.节省内存。叶子节点放在磁盘分区2. 在对树进行插入或查找操作很快。3.一遍扫描数据库即可建树。4.可识别噪声点。5. 可作为其他聚类算法的预处理过程BIRCH缺点：1.结果依赖于数据点的插入顺序。2.对非球状的簇聚类效果不好。3.对高维数据聚类效果不好。4.最后得出来的簇可能和自然簇相差很大。5.在整个过程中算法一旦中断，一切必须从头再来。6.局部性 *BUBBLE 把BIRCH算法的中心和半径概念推广到普通的距离空间   *BUBBLE-FM 通过减少距离的计算次数，提高了BUBBLE算法的效率   Probabilistic agglomerative clustering 距离度量用： 如果dist小于零，则合并两个簇。 易于理解一般跟其他凝聚式层次聚类算法的效率差不多但是：it outputs only one hierarchy with respect to a chosen probabilistic model; it cannot handle the uncertainty of cluster hierarchies.</summary></entry><entry><title type="html">聚类算法总结 - Partitional Clustering</title><link href="http://localhost:8080/datamining/2015/09/21/partitinoal-clustering.html" rel="alternate" type="text/html" title="聚类算法总结 - Partitional Clustering" /><published>2015-09-21T10:31:05+08:00</published><updated>2015-09-21T10:31:05+08:00</updated><id>http://localhost:8080/datamining/2015/09/21/partitinoal-clustering</id><content type="html" xml:base="http://localhost:8080/datamining/2015/09/21/partitinoal-clustering.html">&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;算法&lt;/th&gt;
      &lt;th&gt;概括&lt;/th&gt;
      &lt;th&gt;优缺点&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;k-means&lt;/td&gt;
      &lt;td&gt;每次从类中求均值作为中心点&lt;br /&gt;用到了EM的思想&lt;br /&gt;目标是最小化sum of squared error&lt;/td&gt;
      &lt;td&gt;要求预设k值&lt;br /&gt;易受噪音和离异点的影响 &lt;br /&gt;对不规则形状的类聚类效果不好&lt;br /&gt;不保证全局最优&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;k-means++&lt;/td&gt;
      &lt;td&gt;目标是找到k个合理的初始种子点给k-means。&lt;br /&gt;1. 随机挑个随机点当“种子点”&lt;br /&gt;2. 对于每个点，计算其和最近的“种子点”的距离D(x)并保存，然后把这些距离加起来得到Sum(D(x))。&lt;br /&gt;3. 再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&amp;lt;=0，此时的点就是下一个“种子点”。&lt;br /&gt;4. 重复2和3直到k个中心被选出来&lt;br /&gt;5. 利用这k个初始的聚类中心来运行标准的k-means算法&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;k-modes&lt;/td&gt;
      &lt;td&gt;K-Means算法的扩展&lt;br /&gt;对于分类型数据，用mode求中心点&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;k-prototypes&lt;/td&gt;
      &lt;td&gt;结合了k-means和k-modes&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;k-medoids&lt;/td&gt;
      &lt;td&gt;每次从类中找一个具体的点来做中心点。目标是最小化absolute error。&lt;br /&gt;PAM是一种典型的k-medoids实现。&lt;/td&gt;
      &lt;td&gt;对噪音和离异点不那么敏感&lt;br /&gt;然而计算量大很多&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CLARA&lt;/td&gt;
      &lt;td&gt;先抽样，再用PAM&lt;/td&gt;
      &lt;td&gt;对于大数据比PAM好点&lt;br /&gt;主要是看sample的效果&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CLARANS&lt;/td&gt;
      &lt;td&gt;每次随机的抓一个medoid跟一般点，然后判断，这两者如果替换的话，能不能减小absolute-error&lt;/td&gt;
      &lt;td&gt;融合了PAM和CLARA两者的优点，是第一个用于空间数据库的聚类算法&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name></name></author><summary type="html">算法 概括 优缺点 k-means 每次从类中求均值作为中心点用到了EM的思想目标是最小化sum of squared error 要求预设k值易受噪音和离异点的影响 对不规则形状的类聚类效果不好不保证全局最优 k-means++ 目标是找到k个合理的初始种子点给k-means。1. 随机挑个随机点当“种子点”2. 对于每个点，计算其和最近的“种子点”的距离D(x)并保存，然后把这些距离加起来得到Sum(D(x))。3. 再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其&amp;lt;=0，此时的点就是下一个“种子点”。4. 重复2和3直到k个中心被选出来5. 利用这k个初始的聚类中心来运行标准的k-means算法   k-modes K-Means算法的扩展对于分类型数据，用mode求中心点   k-prototypes 结合了k-means和k-modes   k-medoids 每次从类中找一个具体的点来做中心点。目标是最小化absolute error。PAM是一种典型的k-medoids实现。 对噪音和离异点不那么敏感然而计算量大很多 CLARA 先抽样，再用PAM 对于大数据比PAM好点主要是看sample的效果 CLARANS 每次随机的抓一个medoid跟一般点，然后判断，这两者如果替换的话，能不能减小absolute-error 融合了PAM和CLARA两者的优点，是第一个用于空间数据库的聚类算法</summary></entry><entry><title type="html">《30天自制操作系统》笔记五六</title><link href="http://localhost:8080/learning-note/2014/04/23/30days-os-part56.html" rel="alternate" type="text/html" title="《30天自制操作系统》笔记五六" /><published>2014-04-23T15:22:11+08:00</published><updated>2014-04-23T15:22:11+08:00</updated><id>http://localhost:8080/learning-note/2014/04/23/30days-os-part56</id><content type="html" xml:base="http://localhost:8080/learning-note/2014/04/23/30days-os-part56.html">&lt;p&gt;书上第五六天所涉及的内容主要是GDT、IDT以及PIC。这两部分我就合在一起写好了。（主要的原因是做完第五天的以后看到代码乱得惨不忍睹，就自己开始整理了一下，随后又发现第六天就是讲分割源文件的，所以就继续看了下去。）&lt;/p&gt;

&lt;p&gt;第五天一上来，作者就介绍了结构体。于是，出现了struct boot_info，struct seg_desc，以及struct gate_desc三者。（命名我是按照自己的习惯来改动的。）&lt;/p&gt;

&lt;p&gt;C语言语法上的东西就不提了。&lt;/p&gt;

&lt;p&gt;有点意思的是，继之前恶心简洁的图形化界面之后，我们这次来做文字显示。&lt;/p&gt;

&lt;p&gt;字体怎么来呢？最简单的一种方法就是按照之前做图像的时候的方法来做，也就是用boxfill8()，但是这样也太不专业了吧= =&lt;/p&gt;

&lt;p&gt;我们有一种好一点的方法，就是重写一个putfont8()函数，用来做字符显示。（ps：后来我改名为print_char()了）直接用作者给的hankaku.txt来导入字体。（其实这个字体包也没有多高端嘛……要用到新的工具makefont.exe。可以从hankaku.txt得到hankaku.bin。之后我们再用bin2obj.exe来将其转化为hankaku.obj文件。同时，对应的修改Makefile。&lt;/p&gt;

&lt;p&gt;值得注意的是：一定要加上”$BIN2OBJ ……“的说明！当时就是因为抄漏而导致了大量的失败。&lt;/p&gt;

&lt;p&gt;而显示变量名按照上面的来做基本也没问题。不过不知道为什么，到后面我多加”#include “bootpack.h”“语句之后，直接就只声明“char s[40]”是不够的，要初始化！可以是”char s[40]={‘0’}”。如果没有初始化，就会发现那个字符就是显示不出来，然后你还以为是print_str或者print_char的问题，对照源代码N次！！！（TAT）&lt;/p&gt;

&lt;p&gt;再来是鼠标的指针，这个其实还是蛮无聊有趣的。为了凸显我跟作者的不一样，我把指针弄成了8x8大小的，结果……其实也还是能用的，而且大小我（我不是处女座……）也刚好合适。&lt;/p&gt;

&lt;p&gt;文件分割不难，略过。也就是多出了graphic.c，dsctbl.c，以及后面的int.c。&lt;/p&gt;

&lt;p&gt;有一点是新学习到的，那就是Makefile的一般规则：&lt;/p&gt;

&lt;h1 id=&quot;general-rules-for-gas-nas-and-obj&quot;&gt;general rules for *.gas, *.nas and *.obj&lt;/h1&gt;
&lt;p&gt;%.gas : %.c Makefile
	$(CC1) -o $&lt;em&gt;.gas $&lt;/em&gt;.c
%.nas : %.gas Makefile
	$(GAS2NASK) $&lt;em&gt;.gas $&lt;/em&gt;.nas
%.obj : %.nas Makefile
	$(NASK) $&lt;em&gt;.nas $&lt;/em&gt;.obj $*.lst
关于Makefile，有个很详细的教程：http://bbs.chinaunix.net/thread-408225-1-1.html
之后得找时间认真学习一下才行。&lt;/p&gt;

&lt;p&gt;本次学习遇到的第一个重难点在于GDT以及IDT。&lt;/p&gt;

&lt;p&gt;其实作者的文笔还是挺好的，写得很顺畅。（虽然到后来发现他有些地方机智的绕掉了……）&lt;/p&gt;

&lt;p&gt;该记住的有：&lt;/p&gt;

&lt;p&gt;GDT大小为2^13*(8B) = 64KB&lt;/p&gt;

&lt;p&gt;/* segment descriptor, 8B &lt;em&gt;/
/&lt;/em&gt; base	       : base_low(2B), base_mid(1B) and base_high(1B)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;limit       : limit_low(1B) and limit_high(1B)&lt;/li&gt;
  &lt;li&gt;access_right: the highest bit is Gbit, when Gbit==1, 
              the unit of limit is PAGE; the highest 4bits are put into the 
      highest 4bits of limit_high. thus, to program, access_right
      is xxxx0000xxxxxxxx; the highest 4bits are “GD00”(used after 
      386), G is Gbit, D means 32-mode or 16-mode.
      lowest 8bits:
      0x00: unused descriptor table
      0x92: for system. RW-.
      0x9a: for system. R-X.
      0xf2: for applications. RW-.
      0xfa: for applications. R-X.
 */
struct seg_desc {
 short 	limit_low;
 short 	base_low;
 char 	base_mid; 
 char 	access_right;
 char 	limit_high; 
 char 	base_high;
};
而后面的gate_desc，类比一下就好。
set_seg_desc，set_gate_desc，init_gdt_idt就慢慢看代码吧。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;C不能直接给GDTR赋值，所以要用load_gdtr，在naskfunc.nas中：&lt;/p&gt;

&lt;p&gt;_load_gdtr:		; void load_gdtr(int limit, int addr);
	; “MOV [ESP+6] [ESP+4]”
	MOV	AX, [ESP+4]	; limit
	MOV	[ESP+6], AX
	LGDT	[ESP+6]
	RET
看起来，这段函数似乎做的就是”MOV [ESP+6] [ESP+4]”但是为什么要这么做呢？为什么不直接”LGDT [ESP+4]”呢？
这其中可是大有奥秘的。&lt;/p&gt;

&lt;p&gt;首先，GDTR的低16位是段上限，高32位是地址。我们不能直接用MOV来赋值，而只能直接指定一个内存地址，让它去读这48位。想想看，假设我们传的段上限是0x0000ffff，而地址是00270000（事实上我们用的也就是这个）。这时候，地址从ESP+4往高处走是：【FF FF 00 00 00 00 27 00】。但我们希望给GDTR的是这样一部分：【FF FF 00 00 27 00】那可以看到，作者在这里把ESP+4赋值到ESP+6，然后就变成了【FF FF FF FF 00 00 27 00】，只要直接从ESP+6开始读就可以了。&lt;/p&gt;

&lt;p&gt;哎，在这里实在不得不感叹一声作者太神了！事实上如果你传参数的时候，两个参数的位置如果换一下的话，你会发现很不好处理！（我一开始是想这么干来着。）&lt;/p&gt;

&lt;p&gt;set_seg_desc太高深，跳过。（作者也没讲多少）&lt;/p&gt;

&lt;p&gt;然后就到PIC。一到硬件就各种蛋疼哎！&lt;/p&gt;

&lt;p&gt;PIC指的是Programmable interrupt controller。&lt;/p&gt;

&lt;p&gt;然后看图：&lt;/p&gt;

&lt;p&gt;用图比较容易理解。&lt;/p&gt;

&lt;p&gt;简而言之，PIC监视着输入管脚的8个中断信号，只要有一个中断信号进来，就将唯一的输出管脚信号变成ON，并通知CPU。&lt;/p&gt;

&lt;p&gt;然后下面是一段咋看之下很不明觉厉的代码：&lt;/p&gt;

&lt;p&gt;void init_pic(void);&lt;/p&gt;

&lt;p&gt;#define PIC0_ICW1	0x0020
#define PIC0_OCW2	0x0020
#define PIC0_IMR	0x0021
#define PIC0_ICW2	0x0021
#define PIC0_ICW3	0x0021
#define PIC0_ICW4	0x0021
#define PIC1_ICW1	0x00a0
#define PIC1_OCW2	0x00a0
#define PIC1_IMR	0x00a1
#define PIC1_ICW2	0x00a1
#define PIC1_ICW3	0x00a1
#define PIC1_ICW4	0x00a1 
/* Initialization of pic &lt;em&gt;/
void init_pic (void) {
	io_out8(PIC0_IMR,  0xff  ); /&lt;/em&gt; disable all interrupts &lt;em&gt;/
	io_out8(PIC1_IMR,  0xff  ); /&lt;/em&gt; disable all interrupts */&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;io_out8(PIC0_ICW1, 0x11  ); /* edge trigger mode*/
io_out8(PIC0_ICW2, 0x20  ); /* IRQ-7 is received by INT20-27 */
io_out8(PIC0_ICW3, 1 &amp;lt;&amp;lt; 2); /* PIC1 is connected by IRQ2*/
io_out8(PIC0_ICW4, 0x01  ); /* no-buffer mode */

io_out8(PIC1_ICW1, 0x11  ); /* edge trigger mode */
io_out8(PIC1_ICW2, 0x28  ); /* IRQ-15 is received by INT28-2f */
io_out8(PIC1_ICW3, 2     ); /* PIC1 is connected by IRQ2 */
io_out8(PIC1_ICW4, 0x01  ); /* no-buffer mode */

io_out8(PIC0_IMR,  0xfb  ); /* 11111011 disable all except PIC1 */
io_out8(PIC1_IMR,  0xff  ); /* 11111111 disable all interrupt */ } 简单的翻译一下，IMR指的是interrupt mask register，ICW指的是&quot;initial control word&quot;，都是8位寄存器。 IMR中8位分别对应8个IRQ信号，如果某一位为1，则该为对应的信号被屏蔽，PIC就忽略之。
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ICW不一定是16位，因硬件而不同。有4个，分别编号1~4，共有4个字节的数据。&lt;/p&gt;

&lt;p&gt;ICW1和4与硬件有关，忽略之。&lt;/p&gt;

&lt;p&gt;ICW3是有关主从连接的设定，对主PIC而言，第几号IRQ与从PIC相连，是用8位来决定的。如果把这些为全部设为1，则主PIC能驱动8个从PIC。&lt;/p&gt;

&lt;p&gt;不过呢……这个是硬件决定的，我们也无力。&lt;/p&gt;

&lt;p&gt;所以只能改ICW2咯。&lt;/p&gt;

&lt;p&gt;ICW2决定了IRQ以哪一号中断通知CPU。通过PIC用数据信号线传送给CPU“0xcd 0x??”来实现的。这里的0xcd实际上就是调用BIOS时用的INT指令。&lt;/p&gt;

&lt;p&gt;这次以INT0x20~0x2f接收中断信号IRQ0~15而设定的。&lt;/p&gt;

&lt;p&gt;开始看程序，注意鼠标时IRQ12, 键盘是IRQ1。&lt;/p&gt;

&lt;p&gt;void inthandler21(int &lt;em&gt;esp)
/&lt;/em&gt; interrupt from PS/2 keyboard */
{
	struct boot_info *binfo = (struct boot_info *) BOOT_ADDR;
	boxfill8(binfo-&amp;gt;vram, binfo-&amp;gt;scrnx, BLACK, 0, 0, 32 * 8 - 1, 15);
	print_str(binfo-&amp;gt;vram, binfo-&amp;gt;scrnx, 0, 0, WHITE, “INT 21 (IRQ-1) : PS/2 keyboard”);
	for (;;) {
		io_hlt();
	}
}
完了之后还得让它执行IRETD：
_asm_inthandler21:
		PUSH	ES
		PUSH	DS
		PUSHAD
		MOV		EAX,ESP
		PUSH	EAX
		MOV		AX,SS
		MOV		DS,AX
		MOV		ES,AX
		CALL	_inthandler21
		POP		EAX
		POPAD
		POP		DS
		POP		ES
		IRETD
关于栈，不解释。&lt;/p&gt;

&lt;p&gt;这些差不多就是今天全部的内容了。（唉，累……）&lt;/p&gt;</content><author><name></name></author><summary type="html">书上第五六天所涉及的内容主要是GDT、IDT以及PIC。这两部分我就合在一起写好了。（主要的原因是做完第五天的以后看到代码乱得惨不忍睹，就自己开始整理了一下，随后又发现第六天就是讲分割源文件的，所以就继续看了下去。）</summary></entry></feed>